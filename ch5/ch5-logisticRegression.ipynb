{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ch5 logistic回归\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "定义sigmoid函数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))"
   ]
  },
  {
   "source": [
    "梯度上升法\n",
    "$$\n",
    "w:=w+\\alpha \\Delta_w f(w)\n",
    "$$\n",
    "\n",
    "其中\n",
    "$$\n",
    "\\Delta f(x,y)=\\frac{df(x,y)}{dx},\\frac{df(x,y)}{dy}\n",
    "$$\n",
    "\n",
    "求f(x,y)最大值，用梯度上升，求最小值，用梯度下降，区别在于加个负号。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradAscent(traindataArr,trainlabelArr):\n",
    "    # 梯度上升算法，一次迭代遍历所有数据文件\n",
    "    \"\"\"\n",
    "    参数：训练集，标签集\n",
    "    return： weights\n",
    "    \"\"\"\n",
    "    # w_0*x_0+w_1*x_1+w_2*x_2+...\n",
    "    # 填充x1=1,则w_0等价于截距b\n",
    "    for i in range(len(traindataArr)):\n",
    "        traindataArr[i].append(1)\n",
    "    \n",
    "    # 转为数组类型，方便计算\n",
    "    traindataArr=np.array(traindataArr)\n",
    "    trainlabelArr=np.array(trainlabelArr)\n",
    "    # shape为(100,)与shape为（100,1)区别：\n",
    "    # 前者在矩阵运算时候可能发生广播，所以要reshape一下\n",
    "    trainlabelArr=trainlabelArr.reshape(trainlabelArr.shape[0],1)\n",
    "    \n",
    "    # 初始化参数w，维度为(样本维数,1),即列向量\n",
    "    # w=np.zeros(traindataArr.shape[1]),维度为：(1,样本维数)\n",
    "    w=np.ones( (traindataArr.shape[1],1) )\n",
    "    \n",
    "    # 步长\n",
    "    alpha=0.001\n",
    "    # 迭代次数\n",
    "    iter=500\n",
    "\n",
    "    # iter次随机梯度上升\n",
    "    for i in range(iter):\n",
    "        # print(\"iter:%d /%d\"%(i,iter))\n",
    "        # 每一次迭代，都遍历所有样本\n",
    "        \n",
    "        h=sigmoid(np.dot(traindataArr,w))\n",
    "        error=trainlabelArr-h\n",
    "        w=w+ alpha* np.dot(traindataArr.T,error)\n",
    "    # 返回weights \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 2.78765865],\n",
       "       [ 0.58073184],\n",
       "       [-0.79961773],\n",
       "       [ 2.78765865]])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "# 解析数据\n",
    "def loadDataSet():\n",
    "    '''\n",
    "    Desc: \n",
    "        加载并解析数据\n",
    "    Args:\n",
    "        file_name -- 要解析的文件路径\n",
    "    Returns:\n",
    "        dataMat -- 原始数据的特征\n",
    "        labelMat -- 原始数据的标签，也就是每条样本对应的类别。即目标向量\n",
    "    '''\n",
    "    # dataMat为原始数据， labelMat为原始数据的标签\n",
    "    dataMat = []\n",
    "    labelMat = []\n",
    "    fr = open(\"testSet.txt\")\n",
    "    for line in fr.readlines():\n",
    "        lineArr = line.strip().split()\n",
    "        # 为了方便计算，我们将 X0 的值设为 1.0 ，也就是在每一行的开头添加一个 1.0 作为 X0\n",
    "        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])\n",
    "        labelMat.append(int(lineArr[2]))\n",
    "    return dataMat, labelMat\n",
    "arr,label=loadDataSet()\n",
    "w=gradAscent(arr,label)\n",
    "w"
   ]
  },
  {
   "source": [
    "## 5-4 随机梯度上升\n",
    "\n",
    "梯度上升法在更新回归系数的时候，需要遍历整个数据集。\n",
    "\n",
    "当数据集特征增多时，该方法计算复杂度也会明显增加。\n",
    "\n",
    "随机梯度法是一次仅用一个样本来更新回归系数。由于可以在新样本之前，\n",
    "\n",
    "对分类器进行增量式更新，所以随机梯度下降算法是一个在线学习算法。\n",
    "\n",
    "相对的，一次处理所有数据的方法称为“批处理”方法。\n",
    "\n",
    "随机梯度算法概述：\n",
    "\n",
    "```\n",
    "所有回归系数初始化为1\n",
    "遍历每个样本：\n",
    "    计算该样本梯度\n",
    "    更新回归系数\n",
    "返回回归系数\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randGradAscent(traindataArr,trainlabelArr):\n",
    "    # 随机梯度上升，每遍历一个数据文件就更新一次参数\n",
    "    \"\"\"\n",
    "    参数：训练集，标签集\n",
    "    return： weights\n",
    "    \"\"\"\n",
    "    # 填充x1=1,则w_0等价于截距b\n",
    "    for i in range(len(traindataArr)):\n",
    "        traindataArr[i].append(1)\n",
    "    # 转为数组类型，方便计算\n",
    "    traindataArr=np.array(traindataArr)\n",
    "    trainlabelArr=np.array(trainlabelArr)\n",
    "    # shape为(100,),  取值trainlabelArr[1]=1.0\n",
    "    # shape为(100,1), 取值trainlabelArr[1]=[1.0]\n",
    "    # 注意类型\n",
    "    trainlabelArr=trainlabelArr.reshape(trainlabelArr.shape[0],1)\n",
    "    \n",
    "    # 初始化参数w，维度为(样本维数,1),即列向量\n",
    "    w=np.ones( (traindataArr.shape[1],1) )\n",
    "    # 步长\n",
    "    alpha=0.001\n",
    "    # 迭代次数\n",
    "    iter=500\n",
    "    for i in range(iter):\n",
    "        for j in range(traindataArr.shape[0]):\n",
    "            alpha=4/(1.0+j+i)+0.01\n",
    "            xi=traindataArr[j]\n",
    "            yi=trainlabelArr[j]\n",
    "            # 将(m,)reshape为(1,m)\n",
    "            xi=xi.reshape(1,xi.shape[0])\n",
    "            # w*xi的值，即y_hat\n",
    "            h=sigmoid(np.dot(xi,w))\n",
    "            # err=y-y_hat\n",
    "            error=yi-h\n",
    "            # 更新参数\n",
    "            w = w + alpha* np.dot(xi.T,error)\n",
    "    # 返回weights \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 9.98925941],\n",
       "       [ 1.52852944],\n",
       "       [-2.76149001],\n",
       "       [ 9.98925941]])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# test\n",
    "arr,label=loadDataSet()\n",
    "w=randGradAscent(arr,label)\n",
    "w"
   ]
  },
  {
   "source": [
    "## 5-5 用logistic进行分类\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,x):\n",
    "    # 参数列表，输入数据\n",
    "    # return 类别\n",
    "    wx=np.dot(w.T,x)\n",
    "    p=sigmoid(wx)\n",
    "    if p>0.5:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "source": [
    "## 5-6 modelTest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(filename):\n",
    "    fr=open(filename)\n",
    "    dataArr=[];labelArr=[]\n",
    "    for line in fr.readlines():\n",
    "        # 去掉多余空格，\\t分割\n",
    "        currLine=line.strip().split('\\t')\n",
    "        featList=[]\n",
    "        for i in range(21):\n",
    "            featList.append(float(currLine[i]))\n",
    "        # 特征\n",
    "        dataArr.append(featList)\n",
    "        # 标签\n",
    "        labelArr.append(float(currLine[21]))\n",
    "    return dataArr,labelArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTest(testDataArr,testLabelArr,w):\n",
    "    # return accur\n",
    "    # 填充一列，值为1\n",
    "    for i in range(len(testDataArr)):\n",
    "        testDataArr[i].append(1)\n",
    "    # 错误个数\n",
    "    errCnt=0\n",
    "    for i in range(len(testDataArr)):\n",
    "        if testLabelArr[i]!=predict(w,testDataArr[i]):\n",
    "            errCnt+=1\n",
    "    # return accur\n",
    "    return 1-errCnt/float(len(testLabelArr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7014925373134329"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "trainData, trainLabel = loadData(\"./horseColicTraining.txt\")\n",
    "testData,testLabel=loadData(\"./horseColicTest.txt\")\n",
    "# 随机梯度下降\n",
    "# w=randGradAscent(trainData,trainLabel)\n",
    "# 梯度下降\n",
    "w=gradAscent(trainData,trainLabel)\n",
    "accu=modelTest(testData,testLabel,w)\n",
    "accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# a=np.ones((2,1))\n",
    "a=np.ones(2)\n",
    "c=a[1]\n",
    "c"
   ]
  }
 ]
}