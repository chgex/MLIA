{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ch4 朴素贝叶斯\n",
    "\n",
    "贝叶斯准则:\n",
    "$$\n",
    "P(B|A)=\\frac{P(A|B)*P(B)}{P(A)}\n",
    "$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 文本分类\n",
    "\n",
    "## 4-1 准备数据：从文本构建词向量\n",
    "\n",
    "考虑文档中所有单词，然后将每篇文档住转换为词汇表上的向量。 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    \"\"\"\n",
    "    创建数据集\n",
    "    :return: 单词列表postingList, 所属类别classVec\n",
    "    \"\"\"\n",
    "    dataList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    labelList = [0, 1, 0, 1, 0, 1] \n",
    "    return dataList, labelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    # 所有单词的集合\n",
    "    vocabList=[]\n",
    "    vocabSet=set([])\n",
    "    for textLine in dataSet:\n",
    "        vocabSet=vocabSet | set(textLine)\n",
    "    vocabList=list(vocabSet)\n",
    "    vocabList.sort()\n",
    "    return vocabList"
   ]
  },
  {
   "source": [
    "## 4-2 创建词袋"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList,textLine):\n",
    "    retVec=[0]*len(vocabList)\n",
    "    for word in textLine:\n",
    "        if word in vocabList:\n",
    "            retVec[vocabList.index(word)]=1\n",
    "    return retVec"
   ]
  },
  {
   "source": [
    "**测试**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['I', 'ate', 'buying', 'cute', 'dalmation']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "dataList,dataLabel=loadData()\n",
    "vocalList=createVocabList(dataList)\n",
    "vocalList[:5]"
   ]
  },
  {
   "source": [
    "setOfWord2Vec(vocalList,dataList[0])"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4-3 计算先验概率"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import numpy as np\n",
    "\n",
    "def getProb(trainData, trainCategory):\n",
    "    # 参数：文本单词矩阵，文本类型\n",
    "    # 样本个数\n",
    "    numTrainDocs = len(trainData)\n",
    "    # 词袋的单词数\n",
    "    numWords = len(trainData[0])\n",
    "    # 样本类别\n",
    "    classNum=2\n",
    "    # 类被为1，先验概率\n",
    "    Py = np.sum(trainCategory) / float(numTrainDocs)\n",
    "    # 条件概率，两个类别，对应两个向量\n",
    "    Px_y=np.ones( (classNum,numWords) )\n",
    "    \n",
    "    # 整个数据集，不同类别下，单词出现的总数\n",
    "    p_sum=np.zeros( (classNum,1) )\n",
    "    p_sum+=2.0\n",
    "    # 矩阵：不同类别下，每个单词出现的概率\n",
    "    pVec=np.zeros( (classNum,numWords) )\n",
    "    \n",
    "    # 遍历每一个文本\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            Px_y[1] += trainData[i] \n",
    "            # 对向量中的所有元素进行求和，\n",
    "            # 即计算类别为1的文件中，出现的单词的总数\n",
    "            p_sum[1] += np.sum(trainData[i])\n",
    "        else:\n",
    "            # 类别为0\n",
    "            Px_y[0] += trainData[i]\n",
    "            p_sum[0] += np.sum(trainData[i])\n",
    "    # 每个类别下，每个单词出现的概率\n",
    "    # 如，类别0，正常文档，[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]列表\n",
    "    pVec=np.log(Px_y / p_sum)\n",
    "\n",
    "    return pVec, Py   "
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": []
  },
  {
   "source": [
    "## 4-4 分类"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(textVec, pVec, pClass1):\n",
    "    # 两个类别\n",
    "    # P(w|c1) * P(c1)，即贝叶斯准则的分子\n",
    "    p1 = np.sum(textVec * pVec[1]) + np.log(pClass1) \n",
    "    # P(w|c0) * P(c0)\n",
    "    p0 = np.sum(textVec * pVec[0]) + np.log(1.0 - pClass1) \n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "source": [
    "## 4-5 测试"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def testing():\n",
    "    # 1. 加载数据集\n",
    "    listOPosts, listClasses = loadData()\n",
    "    # 2. 创建单词集合\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    # 3. 计算单词是否出现并创建数据矩阵\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        # 返回m*len(myVocabList)的矩阵， 记录的都是0，1信息\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    # 4. 训练数据\n",
    "    pV, pAb = getProb(np.array(trainMat), np.array(listClasses))\n",
    "    print('p0V',pV)\n",
    "    print('pAb',pAb)\n",
    "    \n",
    "    # 5. 测试数据\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    \n",
    "    print (testEntry, 'classified as: ', classify(thisDoc, pV, pAb))\n",
    "    \n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    \n",
    "    print (testEntry, 'classified as: ', classify(thisDoc, pV, pAb))\n",
    "testing()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "p0V [[-2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936 -2.56494936\n  -2.56494936 -3.25809654 -3.25809654 -2.56494936 -2.56494936 -2.15948425\n  -2.56494936 -2.56494936 -2.56494936 -2.56494936 -3.25809654 -2.56494936\n  -1.87180218 -3.25809654 -3.25809654 -2.56494936 -3.25809654 -2.56494936\n  -3.25809654 -2.56494936 -2.56494936 -2.56494936 -3.25809654 -3.25809654\n  -2.56494936 -3.25809654]\n [-3.04452244 -3.04452244 -2.35137526 -3.04452244 -3.04452244 -1.94591015\n  -3.04452244 -2.35137526 -2.35137526 -3.04452244 -3.04452244 -2.35137526\n  -3.04452244 -3.04452244 -3.04452244 -3.04452244 -2.35137526 -3.04452244\n  -3.04452244 -2.35137526 -2.35137526 -3.04452244 -2.35137526 -3.04452244\n  -2.35137526 -3.04452244 -3.04452244 -2.35137526 -1.65822808 -2.35137526\n  -2.35137526 -1.94591015]]\npAb 0.5\n['love', 'my', 'dalmation'] classified as:  0\n['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ]
  },
  {
   "source": [
    "## 4-6 使用朴素贝叶斯进行邮件分类"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "将文本文件解析成词条向量"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['this', 'book', 'the', 'best', 'book', 'python']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "def textSplit(text):\n",
    "    import re\n",
    "    retList=[]\n",
    "    reg = re.compile('\\W')\n",
    "    wordList = reg.split(text)\n",
    "    retList=[word.lower() for word in wordList if len(word)>2]\n",
    "    return retList\n",
    "# f=open('./email/ham/1.txt').read()\n",
    "f='this book is the best book on python.'\n",
    "d=textSplit(f)\n",
    "d"
   ]
  },
  {
   "source": [
    "测试"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    dataList=[]\n",
    "    labelList=[]\n",
    "    # 训练集 ：测试集合,划分比例：rate\n",
    "    rate=0.7\n",
    "    for i in range(1, 23):\n",
    "        # spam\n",
    "        wordList = textSplit(open('./email/spam/%d.txt' % i).read())\n",
    "        dataList.append(wordList)\n",
    "        labelList.append(1)\n",
    "        # ham\n",
    "        wordList = textSplit(open('./email/ham/%d.txt' % i).read())\n",
    "        dataList.append(wordList)\n",
    "        labelList.append(0)\n",
    "    # train\n",
    "    trainDataArr=dataList[:int(rate*len(dataList))]\n",
    "    trainLabelArr=labelList[:int(rate*len(dataList))]\n",
    "    # test\n",
    "    testDataArr=dataList[int(rate*len(dataList)):]\n",
    "    testLabelArr=labelList[int(rate*len(dataList)):]\n",
    "\n",
    "    return trainDataArr,trainLabelArr,testDataArr,testLabelArr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2Mat(trainDataArr,trainLabelArr):\n",
    "    # 文本数据转为词向量矩阵\n",
    "    # 创建词汇表    \n",
    "    vocabList = createVocabList(trainDataArr)\n",
    "    # 文本数据转为词向量矩阵\n",
    "    dataMat = []\n",
    "    labels = []\n",
    "    for i in range(len(trainDataArr)):\n",
    "        dataMat.append(setOfWords2Vec(vocabList,trainDataArr[i] ))\n",
    "        labels.append(trainLabelArr[i])\n",
    "    return dataMat,labels,vocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTest():\n",
    "    \n",
    "    # 加载数据\n",
    "    trainDataArr,trainLabelArr,testDataArr,testLabelArr=loadData()\n",
    "    \n",
    "    # 训练用的文本数据，转为词向量矩阵\n",
    "    trainDataMat,trainLabels ,vocabList= data2Mat(trainDataArr,trainLabelArr)\n",
    "    \n",
    "    pV, pSpam = getProb(np.array(trainDataMat), np.array(trainLabels))\n",
    "    \n",
    "    errorCount = 0\n",
    "    for i in range(len(testDataArr)):\n",
    "        wordVector = setOfWords2Vec(vocabList,testDataArr[i] )\n",
    "        if classify(np.array(wordVector), pV, pSpam) != testLabelArr[i]:\n",
    "            errorCount += 1\n",
    "    print ('the errorCount is: ', errorCount)\n",
    "    print ('the testSet length is :', len(testLabelArr))\n",
    "    print ('the accu is: :', 1.0-float(errorCount)/len(testLabelArr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the errorCount is:  1\nthe testSet length is : 14\nthe accu is: : 0.9285714285714286\n"
     ]
    }
   ],
   "source": [
    "modelTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}