{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch3-决策树\n",
    "\n",
    "## 3.1-信息熵\n",
    "\n",
    "熵定义为信息的期望值，为了计算熵，我们计算所有类别可能值包含的信息期望值，通过以下公式：\n",
    "$$\n",
    "H=-\\sum_{i=1}^{n}p(x_i)log_2(p(x_i))\n",
    "$$\n",
    "其中，n是分类数目。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 函数：计算给定数据集的熵"
   ]
  },
  {
   "source": [
    "import math\n",
    "def calcuEnt(dataset):\n",
    "    # 样本个数\n",
    "    numOfDat=len(dataset)\n",
    "    # 字典： {标签:出现次数}\n",
    "    labels={}\n",
    "    for featVec in dataset:\n",
    "        #dataset最后一列是label列，即类别\n",
    "        currLabel=featVec[-1]\n",
    "        if currLabel not in labels.keys():\n",
    "            labels[currLabel]=0\n",
    "        labels[currLabel]+=1\n",
    "    # 计算熵\n",
    "    ent=0.0\n",
    "    for key in labels:\n",
    "        # 概率\n",
    "        p=float(labels[key])/numOfDat\n",
    "        # 熵\n",
    "        ent-=p*math.log(p,2)\n",
    "    # 返回该数据集的熵\n",
    "    return ent"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "source": [],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2 划分数据集\n",
    "\n",
    "对每个特征所划分的数据集的结果计算一次信息熵，选取划分最好的特征。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 函数：按照给定特征划分数据集"
   ]
  },
  {
   "source": [
    "def splitDat(dataset,axis,value):\n",
    "    # 划分之后的数据集\n",
    "    retDat=[]\n",
    "    for i in range(len(dataSet)):\n",
    "        # 如果第i个样本的第axis个特征值，为value，则跳过该值\n",
    "        if dataSet[i][axis]==value: \n",
    "            retDat.append(dataSet[i][0:axis] + dataSet[i][axis+1:])\n",
    "    return retDat"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataSet(dataSet,axis,value): # 按某个特征分类后的数据\n",
    "    retDataSet=[]\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis]==value:\n",
    "            reducedFeatVec =featVec[0:axis]\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## 3-3 选择最好划分方式下的特征\n",
    "\n",
    "遍历整个数据集，循环使用计算熵和划分数据集这两个函数，\n",
    "\n",
    "根据熵计算，找到最好的特征。"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "def chooseBestFeature(dataset):\n",
    "    # 样本的特征数,\n",
    "    # 最后一列是label\n",
    "    numOfFeature=len(dataset[0])-1\n",
    "    # 熵\n",
    "    preEnt=calcuEnt(dataSet)\n",
    "    # 最优划分下的熵增益值和特征\n",
    "    bestGain=0.0\n",
    "    bestFeature=-1\n",
    "    for i in range(numOfFeature):\n",
    "        # 保存特征\n",
    "        featList=[featVec[i] for featVec in dataSet]\n",
    "        # 去重复\n",
    "        uniqueList=set(featList)\n",
    "        # 第i个特征划分，数据集的信息熵 \n",
    "        newEnt=0.0\n",
    "        for value in uniqueList:\n",
    "            # 按照value,进行数据集的划分\n",
    "            subDat=splitDataSet(dataSet,i,value)\n",
    "            # 计算每个value划分的prob和信息熵\n",
    "            p=len(subDat)/float(len(dataSet))\n",
    "            # 信息熵\n",
    "            newEnt+=p*calcuEnt(subDat)\n",
    "        # 判断是否需要为最优划分\n",
    "        # 信息熵增益\n",
    "        entGain=preEnt-newEnt\n",
    "        if (entGain>bestGain):\n",
    "            bestGain=entGain\n",
    "            bestFeature=i \n",
    "    # 返回bestFeature,bestGain\n",
    "    return bestFeature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def chooseBestFeature(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    # 数据集的原始信息熵\n",
    "    baseEntropy = calcuEnt(dataSet)\n",
    "    # 最优的信息增益值, 和最优的Featurn编号\n",
    "    bestInfoGain, bestFeature = 0.0, -1\n",
    "    # iterate over all the features\n",
    "    for i in range(numFeatures):\n",
    "        # create a list of all the examples of this feature\n",
    "        # 获取对应的feature下的所有数据\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        # get a set of unique values\n",
    "        # 获取剔重后的集合，使用set对list数据进行去重\n",
    "        uniqueVals = set(featList)\n",
    "        # 创建一个临时的信息熵\n",
    "        newEntropy = 0.0\n",
    "        # 遍历某一列的value集合，计算该列的信息熵 \n",
    "        # 遍历当前特征中的所有唯一属性值，对每个唯一属性值划分一次数据集，计算数据集的新熵值，并对所有唯一特征值得到的熵求和。\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            # 计算概率\n",
    "            prob = len(subDataSet)/float(len(dataSet))\n",
    "            # 计算信息熵\n",
    "            newEntropy += prob * calcuEnt(subDataSet)\n",
    "        # gain[信息增益]: 划分数据集前后的信息变化， 获取信息熵最大的值\n",
    "        # 信息增益是熵的减少或者是数据无序度的减少。最后，比较所有特征中的信息增益，返回最好特征划分的索引值。\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        if (infoGain > bestInfoGain):\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    return bestFeature"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-4 递归构建决策树\n",
    "\n",
    "递归构建决策树：原始数据集，基于最好的feature，划分数据集，由于对应的value可能有多个，所以可能存在大于2个分支的数据集划分。第一次划分之后，数据向下传递到分支的下一个结点，在该节点上继续划分，重复如上流程。\n",
    "\n",
    "递归结束条件：\n",
    "\n",
    "+ 分支下所有实例都具有相同分类；\n",
    "+ 到达叶子结点的数据，属于叶子结点的分类。\n",
    "\n",
    "输入数据集和标签列表，算法运行中并不需要标签列表，为了给数据一个明确的含义，所以作为一个参数提供。"
   ]
  },
  {
   "source": [
    "import operator\n",
    "# 如果数据集已经处理了所有属性，则采用多数表决的方法决定该叶子结点的分类\n",
    "\n",
    "def majorCnt(classList):\n",
    "    classCnt={}\n",
    "    for ticket in classList:\n",
    "        # 如果不在字典，则需要初始化为0\n",
    "        if ticket not in classCnt.keys():\n",
    "            classCnt[ticket]=0\n",
    "        classCnt[ticket]+=1\n",
    "    # 按照出现次数排序\n",
    "    sortedClassCnt=sorted(classCnt.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    # 如果数据集已经处理了所有属性，则采用多数表决的方法决定该叶子结点的分类\n",
    "    return sortedClassCnt[0][0]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": []
  },
  {
   "source": [],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTree(dataSet,labels):\n",
    "    # dataset的最后一列是类别\n",
    "    classList=[example[-1] for example in dataSet]\n",
    "    # print('start creat node: %d/%d'%(len(dataSet[0]),len(classList)))\n",
    "    \n",
    "    # 边界1：类别都相同，则停止划分,直接返回该类别\n",
    "    if classList.count(classList[0])==len(classList):\n",
    "        return classList[0]\n",
    "    # 边界2：如果已经遍历完所有的特征，即没有feature来继续做划分，\n",
    "    # 则采用投票法，返回出现次数最多的类别。\n",
    "    if len(dataSet[0])==1:\n",
    "        return majorCnt(classList)\n",
    "    \n",
    "    # 继续划分，构建子节点\n",
    "    # 最优划分的feature\n",
    "    bestFeat=chooseBestFeature(dataSet) \n",
    "    # 构建样本feature列的信息\n",
    "    bestFeatLabel=labels[bestFeat]\n",
    "    # 字典形式保存树\n",
    "    myTree={bestFeatLabel:{}} \n",
    "    # 去掉该列，因为在之后的划分中，\n",
    "    # 该列不在dataSet中了，需要与labels对应\n",
    "    tmp=labels[:]\n",
    "    del(tmp[bestFeat])\n",
    "    subLabels=tmp[:]\n",
    "    valueList=[example[bestFeat] for example in dataSet]\n",
    "    uniqueVals=set(valueList)\n",
    "    for value in uniqueVals:\n",
    "        # 复制，使操作不影响到原数据\n",
    "        \n",
    "        myTree[bestFeatLabel][value]=createTree(splitDataSet(dataSet,bestFeat,value),subLabels)\n",
    "    return myTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'声音': {'粗': {'头发': {'长': '女', '短': '男'}}, '细': '女'}}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "dataSet = [['长', '粗', '男'],\n",
    "               ['短', '粗', '男'],\n",
    "               ['短', '粗', '男'],\n",
    "               ['长', '细', '女'],\n",
    "               ['短', '细', '女'],\n",
    "               ['短', '粗', '女'],\n",
    "               ['长', '粗', '女'],\n",
    "               ['长', '粗', '女']]\n",
    "labels = ['头发','声音']  #两个特征\n",
    "myTree=createTree(dataSet,labels)\n",
    "myTree"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3-5 测试"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def predict(tree, labels, testVec):\n",
    "    \"\"\"\n",
    "        tree  决策树模型\n",
    "        labels Feature标签对应的名称\n",
    "        testVec    测试输入的数据\n",
    "    Returns: classLabel    分类的结果值，映射label返回名称\n",
    "    \"\"\"\n",
    "    # 根节点对应的key值，\n",
    "    # 即第一个feature\n",
    "    firstStr = list(tree.keys())[0]\n",
    "    # 根节点对应的value值，\n",
    "    # 即根结点的子树\n",
    "    secondDict = tree[firstStr]\n",
    "    # 判断根节点名称获取根节点在label中的先后顺序，这样就知道输入的testVec怎么开始对照树来做分类\n",
    "    # 索引\n",
    "    featIndex = labels.index(firstStr)\n",
    "    key = testVec[featIndex]\n",
    "    valueOfFeat = secondDict[key]\n",
    "    # 判断分枝是否结束: 判断valueOfFeat是否是dict类型\n",
    "    if isinstance(valueOfFeat, dict):\n",
    "        classLabel = predict(valueOfFeat, labels, testVec)\n",
    "    else:\n",
    "        classLabel = valueOfFeat\n",
    "    return classLabel"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 8,
   "outputs": []
  },
  {
   "source": [
    "labe=['头发','声音']\n",
    "predict(myTree,labe,['长', '粗'])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'女'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ]
  },
  {
   "source": [
    "## 3-6 存储决策树"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeTree(tree,filename):\n",
    "    import pickle\n",
    "    fw=open(filename,'wb')\n",
    "    pickle.dump(tree,fw)\n",
    "    fw.close()\n"
   ]
  },
  {
   "source": [
    "def loadTree(filename):\n",
    "    import pickle\n",
    "    fr=open(filename,'rb')\n",
    "    return pickle.load(fr)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 11,
   "outputs": []
  },
  {
   "source": [
    "## 3-7 使用决策树预测隐形眼镜类型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['young', 'myope', 'no', 'reduced', 'no lenses'],\n",
       " ['young', 'myope', 'no', 'normal', 'soft'],\n",
       " ['young', 'myope', 'yes', 'reduced', 'no lenses']]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "fr=open('./lenses.txt')\n",
    "dataList=[example.strip().split('\\t') for example in fr.readlines()]\n",
    "labelList=['age', 'prescript', 'astigmatic', 'tearRate']\n",
    "dataList[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "start create tree\ntree is: {'tearRate': {'reduced': 'no lenses', 'normal': {'astigmatic': {'no': {'age': {'presbyopic': {'prescript': {'hyper': 'soft', 'myope': 'no lenses'}}, 'pre': 'soft', 'young': 'soft'}}, 'yes': 'hard'}}}}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "dataSet=dataList\n",
    "random.shuffle(dataSet)\n",
    "trainDataList=dataSet[:int(0.8*len(dataSet))]\n",
    "testDataList=dataSet[int(0.8*len(dataSet)):]\n",
    "print(\"start create tree\")\n",
    "tree=createTree(trainDataList,labelList)\n",
    "print(\"tree is:\",tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "storeTree(tree,\"lenses_tree.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'tearRate': {'normal': {'astigmatic': {'yes': {'prescript': {'hyper': {'age': {'young': 'hard',\n",
       "        'presbyopic': 'no lenses',\n",
       "        'pre': 'no lenses'}},\n",
       "      'myope': 'hard'}},\n",
       "    'no': {'age': {'young': 'soft',\n",
       "      'presbyopic': {'prescript': {'hyper': 'soft', 'myope': 'no lenses'}},\n",
       "      'pre': 'soft'}}}},\n",
       "  'reduced': 'no lenses'}}"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "myTree=loadTree(\"./lenses_tree.txt\")\n",
    "myTree"
   ]
  },
  {
   "source": [
    "## 3-8 准确率"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def model_test(tree,testDataList, labelList):\n",
    "    #错误数\n",
    "    errorCnt = 0\n",
    "    #遍历测试集\n",
    "    for i in range(len(testDataList)):\n",
    "        if testDataList[i][-1] != predict(tree,labelList,testDataList[i][:-1]):\n",
    "            errorCnt += 1\n",
    "    #返回准确率\n",
    "    return 1 - errorCnt / len(testDataList)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 22,
   "outputs": []
  },
  {
   "source": [
    "#测试准确率\n",
    "labe=['age', 'prescript', 'astigmatic', 'tearRate']\n",
    "accur = model_test(tree,testDataList,labe)\n",
    "print('the accur is:', accur)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the accur is: 0.6\n"
     ]
    }
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "a8216042bf0115a7407236a49e303fe82e47b9d3f1df6a143b06a3096beaf25d"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}